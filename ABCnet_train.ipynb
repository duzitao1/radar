{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 说明\n",
    "用于实现AAnet的训练和测试,使用lightning框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集并划分训练集和验证集\n",
    "需要修改的地方：\n",
    "- file_dir: 数据集的路径\n",
    "- gesture_classes: 手势的类别数\n",
    "- sample_num: 每个类别的样本数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torchsummary import summary\n",
    "import torch.utils.data as data\n",
    "import lightning as L\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "seed_everything(42, workers=True)   # 固定随机种子\n",
    "\n",
    "file_dir = 'out\\\\pyfeature\\\\'\n",
    "file_dir = 'out\\\\test\\\\pyfeature\\\\'\n",
    "gesture_classes = 2\n",
    "sample_num = 100\n",
    "\n",
    "def load_data(file_path, gesture_index):\n",
    "    \"\"\"加载数据\"\"\"\n",
    "    filename = file_path + str(gesture_index) + '.mat'\n",
    "    data = sio.loadmat(filename)\n",
    "    range_profile = torch.tensor(data['range_profile'], dtype=torch.float32)\n",
    "    speed_profile = torch.tensor(data['speed_profile'], dtype=torch.float32)\n",
    "    angle_profile = torch.tensor(data['angle_profile'], dtype=torch.float32)\n",
    "    return range_profile, speed_profile, angle_profile\n",
    "\n",
    "def generate_labels(gesture_class, sample_num):\n",
    "    \"\"\"生成标签\"\"\"\n",
    "    labels = torch.zeros((gesture_class * sample_num, 1))\n",
    "    for i in range(gesture_class):\n",
    "        labels[i * sample_num:(i + 1) * sample_num] = i\n",
    "    enc = OneHotEncoder()\n",
    "    labels = enc.fit_transform(labels).toarray()\n",
    "    return torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# 初始化数据集\n",
    "range_profile = torch.zeros((gesture_classes * sample_num, 30, 64), dtype=torch.float32)\n",
    "speed_profile = torch.zeros((gesture_classes * sample_num, 30, 64), dtype=torch.float32)\n",
    "angle_profile = torch.zeros((gesture_classes * sample_num, 30, 64), dtype=torch.float32)\n",
    "\n",
    "labels = generate_labels(gesture_classes, sample_num)   # 生成标签\n",
    "\n",
    "# 加载数据\n",
    "for i in range(gesture_classes):\n",
    "    range_profile[i * sample_num:(i + 1) * sample_num, :, :], \\\n",
    "    speed_profile[i * sample_num:(i + 1) * sample_num, :, :], \\\n",
    "    angle_profile[i * sample_num:(i + 1) * sample_num, :, :] = load_data(file_dir, i + 1)\n",
    "\n",
    "dataset_loader = data.TensorDataset(range_profile, speed_profile, angle_profile, labels)\n",
    "\n",
    "# 将数据划分为训练集和验证集\n",
    "train_loader, val_loader= data.random_split(dataset_loader, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "# --------------------------------\n",
    "# 步骤 2: 定义 RadarGestureNet\n",
    "# --------------------------------\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn as nn\n",
    "def one_hot_labels(caategorical_labels):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    on_hot_labels = enc.fit_transform(\n",
    "        caategorical_labels.reshape(-1, 1)).toarray()\n",
    "    return on_hot_labels\n",
    "def one_hot_to_label(one_hot):\n",
    "    return torch.argmax(one_hot, dim=1)\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            nn.Conv1d(30, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, gesture_classes),\n",
    "        )\n",
    "\n",
    "\n",
    "class RadarGestureNet(L.LightningModule):\n",
    "    def __init__(self, encoder, gesture_class):\n",
    "        super().__init__()\n",
    "        self.gesture_class = gesture_class\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = encoder\n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.encoder(x2)+self.encoder(x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(z, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        train_accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"train_accuracy\", train_accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.encoder(x2)+self.encoder(x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        val_loss = criterion(z, y)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.encoder(x2)+self.encoder(x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        test_loss = criterion(z, y)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        \n",
    "        accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=3e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1               [-1, 30, 64]           3,840\n",
      "            Conv1d-2               [-1, 64, 64]           1,984\n",
      "            Conv1d-3               [-1, 64, 64]           5,824\n",
      "            Conv1d-4               [-1, 64, 64]           9,664\n",
      "            Conv1d-5               [-1, 64, 64]           1,984\n",
      "   InceptionModule-6               [-1, 64, 64]               0\n",
      "         LayerNorm-7               [-1, 30, 64]           3,840\n",
      "            Conv1d-8               [-1, 64, 64]           1,984\n",
      "            Conv1d-9               [-1, 64, 64]           5,824\n",
      "           Conv1d-10               [-1, 64, 64]           9,664\n",
      "           Conv1d-11               [-1, 64, 64]           1,984\n",
      "  InceptionModule-12               [-1, 64, 64]               0\n",
      "        LayerNorm-13               [-1, 30, 64]           3,840\n",
      "           Conv1d-14               [-1, 64, 64]           1,984\n",
      "           Conv1d-15               [-1, 64, 64]           5,824\n",
      "           Conv1d-16               [-1, 64, 64]           9,664\n",
      "           Conv1d-17               [-1, 64, 64]           1,984\n",
      "  InceptionModule-18               [-1, 64, 64]               0\n",
      "          Flatten-19                 [-1, 4096]               0\n",
      "           Linear-20                  [-1, 512]       2,097,664\n",
      "             ReLU-21                  [-1, 512]               0\n",
      "           Linear-22                   [-1, 64]          32,832\n",
      "             ReLU-23                   [-1, 64]               0\n",
      "           Linear-24                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 2,200,514\n",
      "Trainable params: 2,200,514\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 5768.00\n",
      "Forward/backward pass size (MB): 0.55\n",
      "Params size (MB): 8.39\n",
      "Estimated Total Size (MB): 5776.95\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\aio_radar\\.conda\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n"
     ]
    }
   ],
   "source": [
    "# 定义模型(临时)\n",
    "# --------------------------------\n",
    "# 步骤 2: 定义 RadarGestureNet\n",
    "# --------------------------------\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn as nn\n",
    "def one_hot_labels(caategorical_labels):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    on_hot_labels = enc.fit_transform(\n",
    "        caategorical_labels.reshape(-1, 1)).toarray()\n",
    "    return on_hot_labels\n",
    "def one_hot_to_label(one_hot):\n",
    "    return torch.argmax(one_hot, dim=1)\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "            nn.LayerNorm([256, 64]),\n",
    "            nn.Conv1d(30, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, gesture_classes),\n",
    "        )\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.branch1x1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch3x3 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.branch5x5 = nn.Conv1d(in_channels, out_channels, kernel_size=5, padding=2)\n",
    "        self.branch_pool = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch_pool = self.branch_pool(nn.functional.max_pool1d(x, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n",
    "        outputs = [branch5x5]\n",
    "        \n",
    "        outputs = torch.cat(outputs, 1)  # Concatenate along the channel dimension\n",
    "        return outputs\n",
    "\n",
    "class RadarGestureNet(L.LightningModule):\n",
    "    def __init__(self, encoder, gesture_class):\n",
    "        super().__init__()\n",
    "        self.gesture_class = gesture_class\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.Icp1 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            InceptionModule(30, 64)\n",
    "        )\n",
    "        \n",
    "        self.Icp2 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            InceptionModule(30, 64)\n",
    "        )\n",
    "        self.Icp3 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            InceptionModule(30, 64)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "    def forward(self, x1, x2, x3):\n",
    "        embedding = self.Icp1(x1)+self.Icp2(x2)+self.Icp3(x3)\n",
    "        \n",
    "        embedding = self.decoder(embedding)\n",
    "        return embedding\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(z, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        train_accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"train_accuracy\", train_accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        val_loss = criterion(z, y)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        test_loss = criterion(z, y)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        \n",
    "        accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "model = RadarGestureNet(encoder=encoder, gesture_class=gesture_classes)\n",
    "summary(model, input_size=[(30, 64),(30, 64),(30, 64)],device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1               [-1, 30, 64]           3,840\n",
      "            Conv1d-2               [-1, 64, 64]           1,984\n",
      "            Conv1d-3               [-1, 64, 64]           5,824\n",
      "            Conv1d-4               [-1, 64, 64]           9,664\n",
      "            Conv1d-5               [-1, 64, 64]           1,984\n",
      "   InceptionModule-6               [-1, 64, 64]               0\n",
      "         LayerNorm-7               [-1, 30, 64]           3,840\n",
      "            Conv1d-8               [-1, 64, 64]           1,984\n",
      "            Conv1d-9               [-1, 64, 64]           5,824\n",
      "           Conv1d-10               [-1, 64, 64]           9,664\n",
      "           Conv1d-11               [-1, 64, 64]           1,984\n",
      "  InceptionModule-12               [-1, 64, 64]               0\n",
      "        LayerNorm-13               [-1, 30, 64]           3,840\n",
      "           Conv1d-14               [-1, 64, 64]           1,984\n",
      "           Conv1d-15               [-1, 64, 64]           5,824\n",
      "           Conv1d-16               [-1, 64, 64]           9,664\n",
      "           Conv1d-17               [-1, 64, 64]           1,984\n",
      "  InceptionModule-18               [-1, 64, 64]               0\n",
      "          Flatten-19                 [-1, 4096]               0\n",
      "           Linear-20                  [-1, 512]       2,097,664\n",
      "             ReLU-21                  [-1, 512]               0\n",
      "           Linear-22                   [-1, 64]          32,832\n",
      "             ReLU-23                   [-1, 64]               0\n",
      "           Linear-24                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 2,200,514\n",
      "Trainable params: 2,200,514\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 5768.00\n",
      "Forward/backward pass size (MB): 0.55\n",
      "Params size (MB): 8.39\n",
      "Estimated Total Size (MB): 5776.95\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\aio_radar\\.conda\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "k:\\aio_radar\\.conda\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "# -------------------\n",
    "# 步骤 3: 训练\n",
    "# -------------------\n",
    "# autoencoder = LitAutoEncoder()\n",
    "\n",
    "model = RadarGestureNet(encoder=encoder, gesture_class=gesture_classes)\n",
    "summary(model, input_size=[(30, 64),(30, 64),(30, 64)],device=\"cpu\")\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=75,\n",
    "    log_every_n_steps=1,\n",
    "    deterministic=True,\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False,\n",
    "    )\n",
    "\n",
    "train_data_loader = data.DataLoader(train_loader, batch_size=512, shuffle=True)\n",
    "val_data_loader = data.DataLoader(val_loader, batch_size=512, shuffle=False)\n",
    "\n",
    "trainer.fit(model, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9750000238418579     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.012651989236474037    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9750000238418579    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.012651989236474037   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.012651989236474037, 'accuracy': 0.9750000238418579}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证损失\n",
    "# trainer.test(model, data.DataLoader(train_loader,batch_size=256))\n",
    "trainer.test(model, data.DataLoader(val_loader,batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "# model_path = r'K:\\aio_radar\\lightning_logs\\version_45\\checkpoints\\epoch=74-step=75.ckpt'\n",
    "# model = RadarGestureNet.load_from_checkpoint(model_path).to(\"cpu\")\n",
    "\n",
    "# # 预测\n",
    "predictions = model(torch.rand(range_profile.shape[0],30,64),torch.rand(range_profile.shape[0],30,64),torch.rand(range_profile.shape[0],30,64))\n",
    "# \n",
    "# predictions = model(torch.rand(1,30,64),torch.rand(1,30,64),torch.rand(1,30,64))\n",
    "\n",
    "\n",
    "predictions = model(range_profile, speed_profile, angle_profile)\n",
    "\n",
    "# # 准确率\n",
    "print(torch.sum(one_hot_to_label(predictions) == one_hot_to_label(labels)).item() / len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1704, 0.0482]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0.1704, 0.0482], grad_fn=<SelectBackward0>)\n",
      "tensor(0.0482, grad_fn=<SelectBackward0>)\n",
      "tensor([0])\n",
      "0.1704249382019043\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "print(predictions[0])\n",
    "print(predictions[0][1])\n",
    "print(one_hot_to_label(predictions))\n",
    "print(predictions[0][one_hot_to_label(predictions)].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(range_profile.shape)\n",
    "# # 绘制range_profile\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(range_profile[0,3,:])\n",
    "# for i in range(range_profile.shape[0]):\n",
    "#     for j in range(range_profile.shape[1]):\n",
    "#         plt.plot(range_profile[i,j,:])\n",
    "# plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
