{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 说明\n",
    "用于实现AAnet的训练和测试,使用lightning框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集并划分训练集和验证集\n",
    "需要修改的地方：\n",
    "- file_dir: 数据集的路径\n",
    "- gesture_classes: 手势的类别数\n",
    "- sample_num: 每个类别的样本数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "C:\\Users\\Du\\AppData\\Local\\Temp\\ipykernel_11788\\74597640.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12000, 10])\n",
      "out\\1000\\1.mat\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'out\\\\1000\\\\1.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mk:\\aio_radar\\.conda\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'out\\\\1000\\\\1.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 加载数据\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(gesture_classes):\n\u001b[0;32m     50\u001b[0m     range_profile[i \u001b[38;5;241m*\u001b[39m sample_num:(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m sample_num, :, :], \\\n\u001b[0;32m     51\u001b[0m     speed_profile[i \u001b[38;5;241m*\u001b[39m sample_num:(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m sample_num, :, :], \\\n\u001b[1;32m---> 52\u001b[0m     angle_profile[i \u001b[38;5;241m*\u001b[39m sample_num:(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m sample_num, :, :] \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(range_profile\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     56\u001b[0m range_profile, speed_profile, angle_profile, labels \u001b[38;5;241m=\u001b[39m load_data(file_dir, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmti\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(file_path, gesture_index)\u001b[0m\n\u001b[0;32m     19\u001b[0m filename \u001b[38;5;241m=\u001b[39m file_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(gesture_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(filename)\n\u001b[1;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m range_profile \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrange_profile\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     23\u001b[0m speed_profile \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeed_profile\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mk:\\aio_radar\\.conda\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    226\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[1;32mk:\\aio_radar\\.conda\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mk:\\aio_radar\\.conda\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[1;32mk:\\aio_radar\\.conda\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'out\\\\1000\\\\1.mat'"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torchsummary import summary\n",
    "import torch.utils.data as data\n",
    "import lightning as L\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "seed_everything(42, workers=True)   # 固定随机种子\n",
    "\n",
    "file_dir = 'out\\\\pyfeature\\\\'\n",
    "file_dir = 'out\\\\test\\\\pyfeature\\\\'\n",
    "file_dir = 'out\\\\1000\\\\'\n",
    "gesture_classes = 4\n",
    "sample_num = 3000\n",
    "\n",
    "def load_data(file_path, gesture_index):\n",
    "    \"\"\"加载数据\"\"\"\n",
    "    filename = file_path + str(gesture_index) + '.mat'\n",
    "    print(filename)\n",
    "    data = sio.loadmat(filename)\n",
    "    range_profile = torch.tensor(data['range_profile'], dtype=torch.float32)\n",
    "    speed_profile = torch.tensor(data['speed_profile'], dtype=torch.float32)\n",
    "    angle_profile = torch.tensor(data['angle_profile'], dtype=torch.float32)\n",
    "    return range_profile, speed_profile, angle_profile\n",
    "\n",
    "def generate_labels(gesture_class, sample_num):\n",
    "    \"\"\"生成标签\"\"\"\n",
    "    labels = torch.zeros((gesture_class * sample_num, 1))\n",
    "    for i in range(gesture_class):\n",
    "        labels[i * sample_num:(i + 1) * sample_num] = i\n",
    "    enc = OneHotEncoder()\n",
    "    labels = enc.fit_transform(labels).toarray()\n",
    "    # 加两列0，使得标签的维度和输出的维度一致\n",
    "    # labels = torch.cat((torch.tensor(labels, dtype=torch.float32), torch.zeros((gesture_class * sample_num, 2), dtype=torch.float32)), 1)\n",
    "    # 加6列0，使得标签的维度和输出的维度一致\n",
    "    labels = torch.cat((torch.tensor(labels, dtype=torch.float32), torch.zeros((gesture_class * sample_num, 6), dtype=torch.float32)), 1)\n",
    "    \n",
    "    return torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# 初始化数据集\n",
    "range_profile = torch.zeros((gesture_classes * sample_num, 30, 64), dtype=torch.float32)\n",
    "speed_profile = torch.zeros((gesture_classes * sample_num, 30, 64), dtype=torch.float32)\n",
    "angle_profile = torch.zeros((gesture_classes * sample_num, 30, 64), dtype=torch.float32)\n",
    "\n",
    "labels = generate_labels(gesture_classes, sample_num)   # 生成标签\n",
    "print(labels.shape)\n",
    "# 加载数据\n",
    "for i in range(gesture_classes):\n",
    "    range_profile[i * sample_num:(i + 1) * sample_num, :, :], \\\n",
    "    speed_profile[i * sample_num:(i + 1) * sample_num, :, :], \\\n",
    "    angle_profile[i * sample_num:(i + 1) * sample_num, :, :] = load_data(file_dir, i + 1)\n",
    "\n",
    "print(range_profile.shape)\n",
    "\n",
    "range_profile, speed_profile, angle_profile, labels = load_data(file_dir, ['avg', 'mti', 'None'])\n",
    "\n",
    "dataset_loader = data.TensorDataset(range_profile, speed_profile, angle_profile, labels)\n",
    "\n",
    "# 将数据划分为训练集和验证集\n",
    "# train_loader, val_loader= data.random_split(dataset_loader, [0.8, 0.2])\n",
    "# 将数据划分为训练集,验证集和测试集\n",
    "train_loader, val_loader, test_loader = data.random_split(dataset_loader, [0.7, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "C:\\Users\\Du\\AppData\\Local\\Temp\\ipykernel_11788\\1566842790.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前i: 0 当前j: 0 读取文件： out\\1000\\avg/1.mat 将存放在： 0 到 1000 之间\n",
      "当前i: 0 当前j: 1 读取文件： out\\1000\\mti/1.mat 将存放在： 1000 到 2000 之间\n",
      "当前i: 0 当前j: 2 读取文件： out\\1000\\None/1.mat 将存放在： 2000 到 3000 之间\n",
      "当前i: 1 当前j: 0 读取文件： out\\1000\\avg/2.mat 将存放在： 3000 到 4000 之间\n",
      "当前i: 1 当前j: 1 读取文件： out\\1000\\mti/2.mat 将存放在： 4000 到 5000 之间\n",
      "当前i: 1 当前j: 2 读取文件： out\\1000\\None/2.mat 将存放在： 5000 到 6000 之间\n",
      "当前i: 2 当前j: 0 读取文件： out\\1000\\avg/3.mat 将存放在： 6000 到 7000 之间\n",
      "当前i: 2 当前j: 1 读取文件： out\\1000\\mti/3.mat 将存放在： 7000 到 8000 之间\n",
      "当前i: 2 当前j: 2 读取文件： out\\1000\\None/3.mat 将存放在： 8000 到 9000 之间\n",
      "当前i: 3 当前j: 0 读取文件： out\\1000\\avg/4.mat 将存放在： 9000 到 10000 之间\n",
      "当前i: 3 当前j: 1 读取文件： out\\1000\\mti/4.mat 将存放在： 10000 到 11000 之间\n",
      "当前i: 3 当前j: 2 读取文件： out\\1000\\None/4.mat 将存放在： 11000 到 12000 之间\n",
      "当前i: 4 当前j: 0 读取文件： out\\1000\\avg/5.mat 将存放在： 12000 到 13000 之间\n",
      "当前i: 4 当前j: 1 读取文件： out\\1000\\mti/5.mat 将存放在： 13000 到 14000 之间\n",
      "当前i: 4 当前j: 2 读取文件： out\\1000\\None/5.mat 将存放在： 14000 到 15000 之间\n",
      "torch.Size([15000, 30, 64])\n",
      "torch.Size([15000, 5])\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torchsummary import summary\n",
    "import torch.utils.data as data\n",
    "import lightning as L\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "seed_everything(42, workers=True)   # 固定随机种子\n",
    "\n",
    "# file_dir = 'out\\\\pyfeature\\\\'\n",
    "# file_dir = 'out\\\\test\\\\pyfeature\\\\'\n",
    "# file_dir = 'out\\\\1000\\\\'\n",
    "# gesture_classes = 4\n",
    "# # 目标类别数\n",
    "# target_classes = 10\n",
    "# # 子文件夹\n",
    "# sub_dir_list = ['avg', 'mti', 'None']\n",
    "# # 子文件夹长度\n",
    "# sub_dir_len = len(sub_dir_list)\n",
    "# # 样本数\n",
    "# sample_num = 1000\n",
    "\n",
    "# 设置上面5个参数的函数\n",
    "def set_params(file_dir, gesture_classes, target_classes, sub_dir_list, sample_num):\n",
    "    return file_dir, gesture_classes, target_classes, sub_dir_list, sample_num\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"加载数据\"\"\"\n",
    "    data = sio.loadmat(file_name)\n",
    "    # range_profile = torch.tensor(data['range_profile'], dtype=torch.float32)\n",
    "    # speed_profile = torch.tensor(data['speed_profile'], dtype=torch.float32)\n",
    "    # angle_profile = torch.tensor(data['angle_profile'], dtype=torch.float32)\n",
    "    \n",
    "    # 因为数据原本是complex类型，所以要先取出复数数据,再转化为float32\n",
    "    # range_profile = torch.tensor(data['range_profile'], dtype=torch.complex128)\n",
    "    range_profile = torch.tensor(data['range_profile'], dtype=torch.complex128).abs().float()\n",
    "    speed_profile = torch.tensor(data['speed_profile'], dtype=torch.complex128).abs().float()\n",
    "    angle_profile = torch.tensor(data['angle_profile'], dtype=torch.complex128).abs().float()\n",
    "    \n",
    "    return range_profile, speed_profile, angle_profile\n",
    "\n",
    "def generate_labels(gesture_class, class_sample_num):\n",
    "    \"\"\"生成标签\"\"\"\n",
    "    labels = torch.zeros((gesture_class * class_sample_num, 1))\n",
    "    for i in range(gesture_class):\n",
    "        labels[i * class_sample_num:(i + 1) * class_sample_num] = i\n",
    "    enc = OneHotEncoder()\n",
    "    labels = enc.fit_transform(labels).toarray()\n",
    "    # 加6列0，使得标签的维度和输出的维度一致\n",
    "    labels = torch.cat((torch.tensor(labels, dtype=torch.float32), torch.zeros((gesture_class * class_sample_num, target_classes-gesture_class), dtype=torch.float32)), 1)\n",
    "    return torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# 写data_loader函数，传入文件夹路径，和子文件夹名称列表如(avg, mti, None),返回range_profile, speed_profile, angle_profile, labels\n",
    "def data_loader(file_dir, sub_dir_list):\n",
    "    sub_dir_len = len(sub_dir_list)\n",
    "    \n",
    "    range_profile = torch.zeros((gesture_classes * sample_num*sub_dir_len, 30, 64), dtype=torch.float32)\n",
    "    speed_profile = torch.zeros((gesture_classes * sample_num*sub_dir_len, 30, 64), dtype=torch.float32)\n",
    "    angle_profile = torch.zeros((gesture_classes * sample_num*sub_dir_len, 30, 64), dtype=torch.float32)\n",
    "    labels = generate_labels(gesture_classes, sample_num*sub_dir_len)   # 生成标签\n",
    "    for i in range(gesture_classes):\n",
    "        for j in range(sub_dir_len):\n",
    "            filename = file_dir + sub_dir_list[j] + '/' + str(i + 1) + '.mat'\n",
    "            print(\"当前i:\",i,\"当前j:\",j,\"读取文件：\",filename,\"将存放在：\",i*sample_num*sub_dir_len+j*sample_num,\"到\",i*sample_num*sub_dir_len+(j + 1)*sample_num,\"之间\")\n",
    "            range_profile[i*sample_num*sub_dir_len+j*sample_num:i*sample_num*sub_dir_len+(j + 1)*sample_num, :, :], \\\n",
    "            speed_profile[i*sample_num*sub_dir_len+j*sample_num:i*sample_num*sub_dir_len+(j + 1)*sample_num, :, :], \\\n",
    "            angle_profile[i*sample_num*sub_dir_len+j*sample_num:i*sample_num*sub_dir_len+(j + 1)*sample_num, :, :] = load_data(filename)\n",
    "            \n",
    "    \n",
    "    return range_profile, speed_profile, angle_profile, labels\n",
    "\n",
    "\n",
    "file_dir, gesture_classes, target_classes, sub_dir_list, sample_num = set_params('out\\\\1000\\\\', 5, 5, ['avg','mti','None'], 1000)\n",
    "range_profile, speed_profile, angle_profile, labels = data_loader(file_dir, sub_dir_list)\n",
    "\n",
    "print(range_profile.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "dataset_loader = data.TensorDataset(range_profile, speed_profile, angle_profile, labels)\n",
    "\n",
    "# # 将数据划分为训练集和验证集\n",
    "# # train_loader, val_loader= data.random_split(dataset_loader, [0.8, 0.2])\n",
    "# 将数据划分为训练集,验证集和测试集\n",
    "train_loader, val_loader, test_loader = data.random_split(dataset_loader, [0.7, 0.2, 0.1])\n",
    "\n",
    "# file_dir, gesture_classes, target_classes, sub_dir_list, sample_num = set_params('out\\\\1000\\\\', 4, 10, ['avg_remove'], 2000)\n",
    "# train_range_profile, train_speed_profile, train_angle_profile, train_labels = data_loader(r\"K:/aio_radar/out/1000/\", ['avg_remove'])\n",
    "# dataset_loader = data.TensorDataset(train_range_profile, train_speed_profile, train_angle_profile, train_labels)\n",
    "# train_loader, val_loader = data.random_split(dataset_loader, [0.8, 0.2])\n",
    "\n",
    "# file_dir, gesture_classes, target_classes, sub_dir_list, sample_num = set_params('out\\\\1000\\\\', 4, 10, ['avg'], 1000)\n",
    "# test_range_profile, test_speed_profile, test_angle_profile, test_labels = data_loader(file_dir, sub_dir_list)\n",
    "# test_loader = data.TensorDataset(test_range_profile, test_speed_profile, test_angle_profile, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "# --------------------------------\n",
    "# 步骤 2: 定义 RadarGestureNet\n",
    "# --------------------------------\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn as nn\n",
    "def one_hot_labels(caategorical_labels):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    on_hot_labels = enc.fit_transform(\n",
    "        caategorical_labels.reshape(-1, 1)).toarray()\n",
    "    return on_hot_labels\n",
    "def one_hot_to_label(one_hot):\n",
    "    return torch.argmax(one_hot, dim=1)\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            nn.Conv1d(30, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, gesture_classes),\n",
    "        )\n",
    "\n",
    "\n",
    "class RadarGestureNet(L.LightningModule):\n",
    "    def __init__(self, encoder, gesture_class):\n",
    "        super().__init__()\n",
    "        self.gesture_class = gesture_class\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = encoder\n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.encoder(x2)+self.encoder(x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(z, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        train_accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"train_accuracy\", train_accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.encoder(x2)+self.encoder(x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        val_loss = criterion(z, y)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.encoder(x2)+self.encoder(x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        test_loss = criterion(z, y)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        \n",
    "        accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=3e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1               [-1, 30, 64]           3,840\n",
      "           Dropout-2               [-1, 30, 64]               0\n",
      "            Conv1d-3               [-1, 64, 64]           1,984\n",
      "            Conv1d-4               [-1, 64, 64]           5,824\n",
      "            Conv1d-5               [-1, 64, 64]           9,664\n",
      "            Conv1d-6               [-1, 64, 64]           1,984\n",
      "   InceptionModule-7              [-1, 256, 64]               0\n",
      "         LayerNorm-8               [-1, 30, 64]           3,840\n",
      "           Dropout-9               [-1, 30, 64]               0\n",
      "           Conv1d-10               [-1, 64, 64]           1,984\n",
      "           Conv1d-11               [-1, 64, 64]           5,824\n",
      "           Conv1d-12               [-1, 64, 64]           9,664\n",
      "           Conv1d-13               [-1, 64, 64]           1,984\n",
      "  InceptionModule-14              [-1, 256, 64]               0\n",
      "        LayerNorm-15               [-1, 30, 64]           3,840\n",
      "          Dropout-16               [-1, 30, 64]               0\n",
      "           Conv1d-17               [-1, 64, 64]           1,984\n",
      "           Conv1d-18               [-1, 64, 64]           5,824\n",
      "           Conv1d-19               [-1, 64, 64]           9,664\n",
      "           Conv1d-20               [-1, 64, 64]           1,984\n",
      "  InceptionModule-21              [-1, 256, 64]               0\n",
      "          Flatten-22                [-1, 16384]               0\n",
      "           Linear-23                  [-1, 512]       8,389,120\n",
      "          Dropout-24                  [-1, 512]               0\n",
      "             ReLU-25                  [-1, 512]               0\n",
      "           Linear-26                   [-1, 64]          32,832\n",
      "          Dropout-27                   [-1, 64]               0\n",
      "             ReLU-28                   [-1, 64]               0\n",
      "           Linear-29                   [-1, 10]             650\n",
      "          Softmax-30                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 8,492,490\n",
      "Trainable params: 8,492,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 5768.00\n",
      "Forward/backward pass size (MB): 0.98\n",
      "Params size (MB): 32.40\n",
      "Estimated Total Size (MB): 5801.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 定义模型(临时)\n",
    "# --------------------------------\n",
    "# 步骤 2: 定义 RadarGestureNet\n",
    "# --------------------------------\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn as nn\n",
    "def one_hot_labels(caategorical_labels):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    on_hot_labels = enc.fit_transform(\n",
    "        caategorical_labels.reshape(-1, 1)).toarray()\n",
    "    return on_hot_labels\n",
    "def one_hot_to_label(one_hot):\n",
    "    return torch.argmax(one_hot, dim=1)\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.branch1x1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch3x3 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.branch5x5 = nn.Conv1d(in_channels, out_channels, kernel_size=5, padding=2)\n",
    "        self.branch_pool = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch_pool = self.branch_pool(nn.functional.max_pool1d(x, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n",
    "        \n",
    "        outputs = torch.cat(outputs, 1)  # Concatenate along the channel dimension\n",
    "        return outputs\n",
    "\n",
    "class RadarGestureNet(L.LightningModule):\n",
    "    def __init__(self, gesture_class):\n",
    "        super().__init__()\n",
    "        self.gesture_class = gesture_class\n",
    "        self.save_hyperparameters('gesture_class')\n",
    "        \n",
    "        self.Icp1 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            nn.Dropout(0.6),\n",
    "            InceptionModule(30, 64)\n",
    "        )\n",
    "        \n",
    "        self.Icp2 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            nn.Dropout(0.6),\n",
    "            InceptionModule(30, 64)\n",
    "        )\n",
    "        self.Icp3 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            nn.Dropout(0.6),\n",
    "            InceptionModule(30, 64)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16384, 512),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x1, x2, x3):\n",
    "        embedding = self.Icp1(x1)+self.Icp2(x2)+self.Icp3(x3)\n",
    "        \n",
    "        embedding = self.decoder(embedding)\n",
    "        return embedding\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(z, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        train_accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"train_accuracy\", train_accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        val_loss = criterion(z, y)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        test_loss = criterion(z, y)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        \n",
    "        accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "model = RadarGestureNet(gesture_class=gesture_classes)\n",
    "summary(model, input_size=[(30, 64),(30, 64),(30, 64)],device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1               [-1, 30, 64]           3,840\n",
      "            Conv1d-2               [-1, 30, 64]             120\n",
      "            Conv1d-3               [-1, 15, 64]             465\n",
      "DepthwiseSeparableConv1d-4               [-1, 15, 64]               0\n",
      "            Conv1d-5                [-1, 5, 64]              80\n",
      "            Conv1d-6                [-1, 5, 64]             230\n",
      "            Conv1d-7                [-1, 5, 64]             380\n",
      "            Conv1d-8                [-1, 5, 64]              80\n",
      "   InceptionModule-9               [-1, 20, 64]               0\n",
      "        LayerNorm-10               [-1, 30, 64]           3,840\n",
      "           Conv1d-11               [-1, 30, 64]             120\n",
      "           Conv1d-12               [-1, 15, 64]             465\n",
      "DepthwiseSeparableConv1d-13               [-1, 15, 64]               0\n",
      "           Conv1d-14                [-1, 5, 64]              80\n",
      "           Conv1d-15                [-1, 5, 64]             230\n",
      "           Conv1d-16                [-1, 5, 64]             380\n",
      "           Conv1d-17                [-1, 5, 64]              80\n",
      "  InceptionModule-18               [-1, 20, 64]               0\n",
      "        LayerNorm-19               [-1, 30, 64]           3,840\n",
      "           Conv1d-20               [-1, 30, 64]             120\n",
      "           Conv1d-21               [-1, 15, 64]             465\n",
      "DepthwiseSeparableConv1d-22               [-1, 15, 64]               0\n",
      "           Conv1d-23                [-1, 5, 64]              80\n",
      "           Conv1d-24                [-1, 5, 64]             230\n",
      "           Conv1d-25                [-1, 5, 64]             380\n",
      "           Conv1d-26                [-1, 5, 64]              80\n",
      "  InceptionModule-27               [-1, 20, 64]               0\n",
      "          Flatten-28                 [-1, 1280]               0\n",
      "           Linear-29                  [-1, 640]         819,840\n",
      "          Dropout-30                  [-1, 640]               0\n",
      "             ReLU-31                  [-1, 640]               0\n",
      "           Linear-32                   [-1, 64]          41,024\n",
      "             ReLU-33                   [-1, 64]               0\n",
      "           Linear-34                   [-1, 10]             650\n",
      "          Softmax-35                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 877,099\n",
      "Trainable params: 877,099\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 5768.00\n",
      "Forward/backward pass size (MB): 0.22\n",
      "Params size (MB): 3.35\n",
      "Estimated Total Size (MB): 5771.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 定义模型(深度可分离)\n",
    "# --------------------------------\n",
    "# 步骤 2: 定义 RadarGestureNet\n",
    "# --------------------------------\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn as nn\n",
    "def one_hot_labels(caategorical_labels):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    on_hot_labels = enc.fit_transform(\n",
    "        caategorical_labels.reshape(-1, 1)).toarray()\n",
    "    return on_hot_labels\n",
    "def one_hot_to_label(one_hot):\n",
    "    return torch.argmax(one_hot, dim=1)\n",
    "\n",
    "encoder = 0\n",
    "\n",
    "#\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.branch1x1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch3x3 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.branch5x5 = nn.Conv1d(in_channels, out_channels, kernel_size=5, padding=2)\n",
    "        self.branch_pool = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch_pool = self.branch_pool(nn.functional.max_pool1d(x, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n",
    "        \n",
    "        outputs = torch.cat(outputs, 1)\n",
    "        return outputs\n",
    "\n",
    "# 深度可分离卷积\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super(DepthwiseSeparableConv1d, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size, groups=in_channels, padding=padding)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ABCModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ABCModule, self).__init__()\n",
    "        self.branch3x3 = nn.Conv1d(in_channels, out_channels, kernel_size=5, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        \n",
    "        outputs = [branch3x3]\n",
    "        \n",
    "        outputs = torch.cat(outputs, 1)  # Concatenate along the channel dimension\n",
    "        return outputs\n",
    "\n",
    "class RadarGestureNet(L.LightningModule):\n",
    "    def __init__(self, gesture_class):\n",
    "        super().__init__()\n",
    "        self.gesture_class = gesture_class\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.ABC1 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            DepthwiseSeparableConv1d(30, 15, 3, padding=1),\n",
    "            InceptionModule(15, 5),\n",
    "        )\n",
    "        \n",
    "        self.ABC2 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            DepthwiseSeparableConv1d(30, 15, 3, padding=1),\n",
    "            InceptionModule(15, 5),\n",
    "        )\n",
    "        \n",
    "        self.ABC3 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            DepthwiseSeparableConv1d(30, 15, 3, padding=1),\n",
    "            InceptionModule(15, 5),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, 640),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x1, x2, x3):\n",
    "        embedding = (self.ABC1(x1)+self.ABC2(x3))*self.ABC3(x2)\n",
    "        \n",
    "        embedding = self.decoder(embedding)\n",
    "        return embedding\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        loss = criterion(z, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        train_accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"train_accuracy\", train_accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        val_loss = criterion(z, y)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        test_loss = criterion(z, y)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        \n",
    "        accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "model = RadarGestureNet( gesture_class=gesture_classes)\n",
    "summary(model, input_size=[(30, 64),(30, 64),(30, 64)],device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1               [-1, 30, 64]           3,840\n",
      "            Conv1d-2               [-1, 30, 64]             120\n",
      "            Conv1d-3               [-1, 15, 64]             465\n",
      "DepthwiseSeparableConv1d-4               [-1, 15, 64]               0\n",
      "              ReLU-5               [-1, 15, 64]               0\n",
      "            Conv1d-6               [-1, 10, 64]             160\n",
      "            Conv1d-7               [-1, 10, 64]             460\n",
      "            Conv1d-8               [-1, 10, 64]             760\n",
      "            Conv1d-9               [-1, 10, 64]             160\n",
      "  InceptionModule-10               [-1, 40, 64]               0\n",
      "             ReLU-11               [-1, 40, 64]               0\n",
      "        LayerNorm-12               [-1, 30, 64]           3,840\n",
      "           Conv1d-13               [-1, 30, 64]             120\n",
      "           Conv1d-14               [-1, 15, 64]             465\n",
      "DepthwiseSeparableConv1d-15               [-1, 15, 64]               0\n",
      "             ReLU-16               [-1, 15, 64]               0\n",
      "           Conv1d-17               [-1, 10, 64]             160\n",
      "           Conv1d-18               [-1, 10, 64]             460\n",
      "           Conv1d-19               [-1, 10, 64]             760\n",
      "           Conv1d-20               [-1, 10, 64]             160\n",
      "  InceptionModule-21               [-1, 40, 64]               0\n",
      "             ReLU-22               [-1, 40, 64]               0\n",
      "        LayerNorm-23               [-1, 30, 64]           3,840\n",
      "           Conv1d-24               [-1, 30, 64]             120\n",
      "           Conv1d-25               [-1, 15, 64]             465\n",
      "DepthwiseSeparableConv1d-26               [-1, 15, 64]               0\n",
      "             ReLU-27               [-1, 15, 64]               0\n",
      "           Conv1d-28               [-1, 10, 64]             160\n",
      "           Conv1d-29               [-1, 10, 64]             460\n",
      "           Conv1d-30               [-1, 10, 64]             760\n",
      "           Conv1d-31               [-1, 10, 64]             160\n",
      "  InceptionModule-32               [-1, 40, 64]               0\n",
      "             ReLU-33               [-1, 40, 64]               0\n",
      "          Flatten-34                 [-1, 2560]               0\n",
      "           Linear-35                  [-1, 640]       1,639,040\n",
      "          Dropout-36                  [-1, 640]               0\n",
      "             ReLU-37                  [-1, 640]               0\n",
      "           Linear-38                  [-1, 128]          82,048\n",
      "             ReLU-39                  [-1, 128]               0\n",
      "           Linear-40                    [-1, 5]             645\n",
      "          Softmax-41                    [-1, 5]               0\n",
      "================================================================\n",
      "Total params: 1,739,628\n",
      "Trainable params: 1,739,628\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 5768.00\n",
      "Forward/backward pass size (MB): 0.37\n",
      "Params size (MB): 6.64\n",
      "Estimated Total Size (MB): 5775.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 定义模型(深度可分离)2024_4_3\n",
    "# --------------------------------\n",
    "# 步骤 2: 定义 RadarGestureNet\n",
    "# --------------------------------\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn as nn\n",
    "def one_hot_labels(caategorical_labels):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    on_hot_labels = enc.fit_transform(\n",
    "        caategorical_labels.reshape(-1, 1)).toarray()\n",
    "    return on_hot_labels\n",
    "def one_hot_to_label(one_hot):\n",
    "    return torch.argmax(one_hot, dim=1)\n",
    "\n",
    "encoder = 0\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.branch1x1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch3x3 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.branch5x5 = nn.Conv1d(in_channels, out_channels, kernel_size=5, padding=2)\n",
    "        self.branch_pool = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch_pool = self.branch_pool(nn.functional.max_pool1d(x, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n",
    "        \n",
    "        outputs = torch.cat(outputs, 1)\n",
    "        return outputs\n",
    "\n",
    "# 深度可分离卷积\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super(DepthwiseSeparableConv1d, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size, groups=in_channels, padding=padding)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class ABCModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ABCModule, self).__init__()\n",
    "        self.branch3x3 = nn.Conv1d(in_channels, out_channels, kernel_size=5, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        \n",
    "        outputs = [branch3x3]\n",
    "        \n",
    "        outputs = torch.cat(outputs, 1)  # Concatenate along the channel dimension\n",
    "        return outputs\n",
    "\n",
    "class RadarGestureNet(L.LightningModule):\n",
    "    def __init__(self, gesture_class):\n",
    "        super().__init__()\n",
    "        self.gesture_class = gesture_class\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.ABC1 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            DepthwiseSeparableConv1d(30, 15, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            InceptionModule(15, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.ABC2 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            DepthwiseSeparableConv1d(30, 15, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            InceptionModule(15, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.ABC3 = nn.Sequential(\n",
    "            nn.LayerNorm([30, 64]),\n",
    "            DepthwiseSeparableConv1d(30, 15, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            InceptionModule(15, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2560, 640),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(640, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x1, x2, x3):\n",
    "        embedding = self.ABC1(x2)*(self.ABC2(x1)+self.ABC3(x3))\n",
    "        \n",
    "        embedding = self.decoder(embedding)\n",
    "        return embedding\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        loss = criterion(z, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        train_accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"train_accuracy\", train_accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        val_loss = criterion(z, y)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x1,x2,x3, y = batch\n",
    "        z = self.forward(x1, x2, x3)\n",
    "        criterion = nn.MSELoss()\n",
    "        test_loss = criterion(z, y)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        \n",
    "        accuracy = torch.sum(one_hot_to_label(z) == one_hot_to_label(y)).item() / len(y)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "model = RadarGestureNet( gesture_class=gesture_classes)\n",
    "summary(model, input_size=[(30, 64),(30, 64),(30, 64)],device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1               [-1, 30, 64]           3,840\n",
      "            Conv1d-2               [-1, 30, 64]             120\n",
      "            Conv1d-3               [-1, 15, 64]             465\n",
      "DepthwiseSeparableConv1d-4               [-1, 15, 64]               0\n",
      "              ReLU-5               [-1, 15, 64]               0\n",
      "            Conv1d-6               [-1, 10, 64]             160\n",
      "            Conv1d-7               [-1, 10, 64]             460\n",
      "            Conv1d-8               [-1, 10, 64]             760\n",
      "            Conv1d-9               [-1, 10, 64]             160\n",
      "  InceptionModule-10               [-1, 40, 64]               0\n",
      "             ReLU-11               [-1, 40, 64]               0\n",
      "        LayerNorm-12               [-1, 30, 64]           3,840\n",
      "           Conv1d-13               [-1, 30, 64]             120\n",
      "           Conv1d-14               [-1, 15, 64]             465\n",
      "DepthwiseSeparableConv1d-15               [-1, 15, 64]               0\n",
      "             ReLU-16               [-1, 15, 64]               0\n",
      "           Conv1d-17               [-1, 10, 64]             160\n",
      "           Conv1d-18               [-1, 10, 64]             460\n",
      "           Conv1d-19               [-1, 10, 64]             760\n",
      "           Conv1d-20               [-1, 10, 64]             160\n",
      "  InceptionModule-21               [-1, 40, 64]               0\n",
      "             ReLU-22               [-1, 40, 64]               0\n",
      "        LayerNorm-23               [-1, 30, 64]           3,840\n",
      "           Conv1d-24               [-1, 30, 64]             120\n",
      "           Conv1d-25               [-1, 15, 64]             465\n",
      "DepthwiseSeparableConv1d-26               [-1, 15, 64]               0\n",
      "             ReLU-27               [-1, 15, 64]               0\n",
      "           Conv1d-28               [-1, 10, 64]             160\n",
      "           Conv1d-29               [-1, 10, 64]             460\n",
      "           Conv1d-30               [-1, 10, 64]             760\n",
      "           Conv1d-31               [-1, 10, 64]             160\n",
      "  InceptionModule-32               [-1, 40, 64]               0\n",
      "             ReLU-33               [-1, 40, 64]               0\n",
      "          Flatten-34                 [-1, 2560]               0\n",
      "           Linear-35                  [-1, 640]       1,639,040\n",
      "          Dropout-36                  [-1, 640]               0\n",
      "             ReLU-37                  [-1, 640]               0\n",
      "           Linear-38                  [-1, 128]          82,048\n",
      "             ReLU-39                  [-1, 128]               0\n",
      "           Linear-40                    [-1, 5]             645\n",
      "          Softmax-41                    [-1, 5]               0\n",
      "================================================================\n",
      "Total params: 1,739,628\n",
      "Trainable params: 1,739,628\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 5768.00\n",
      "Forward/backward pass size (MB): 0.37\n",
      "Params size (MB): 6.64\n",
      "Estimated Total Size (MB): 5775.00\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\aio_radar\\.conda\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "k:\\aio_radar\\.conda\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=75` reached.\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "# -------------------\n",
    "# 步骤 3: 训练\n",
    "# -------------------\n",
    "# autoencoder = LitAutoEncoder()\n",
    "\n",
    "model = RadarGestureNet(gesture_class=gesture_classes)\n",
    "summary(model, input_size=[(30, 64),(30, 64),(30, 64)],device=\"cpu\")\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=75,\n",
    "    log_every_n_steps=1,\n",
    "    deterministic=True,\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False,\n",
    "    )\n",
    "\n",
    "train_data_loader = data.DataLoader(train_loader, batch_size=512, shuffle=True)\n",
    "val_data_loader = data.DataLoader(val_loader, batch_size=512, shuffle=False)\n",
    "\n",
    "trainer.fit(model, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\aio_radar\\.conda\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9436666369438171     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.017185678705573082    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9436666369438171    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.017185678705573082   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9446666836738586     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.016482500359416008    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9446666836738586    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.016482500359416008   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.016482500359416008, 'accuracy': 0.9446666836738586}]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证损失\n",
    "trainer.test(model, data.DataLoader(val_loader,batch_size=256))\n",
    "trainer.test(model, data.DataLoader(test_loader,batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "# -------------------\n",
    "# 步骤 4: 保存模型\n",
    "# -------------------\n",
    "# onnx\n",
    "# 保存成pth\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model = RadarGestureNet(gesture_class=gesture_classes)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 30, 64), torch.randn(1, 30, 64), torch.randn(1, 30, 64)\n",
    "torch.onnx.export(model, dummy_input, 'avg_model.onnx', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9118e-01, 5.0276e-05, 1.7751e-07,  ..., 1.0608e-06, 2.2769e-06,\n",
      "         4.1550e-06],\n",
      "        [1.0000e+00, 1.2611e-07, 1.7854e-08,  ..., 8.8934e-10, 7.4204e-09,\n",
      "         5.4652e-08],\n",
      "        [9.9997e-01, 2.8686e-06, 6.1561e-06,  ..., 3.2462e-07, 8.4617e-07,\n",
      "         2.5539e-06],\n",
      "        ...,\n",
      "        [4.6524e-04, 4.8588e-04, 1.3343e-03,  ..., 2.4570e-06, 9.8908e-06,\n",
      "         1.4300e-06],\n",
      "        [9.8021e-03, 2.0627e-03, 3.2580e-07,  ..., 9.9225e-07, 1.0006e-06,\n",
      "         5.3378e-07],\n",
      "        [4.9791e-03, 6.4650e-05, 5.5307e-05,  ..., 1.2257e-06, 2.1895e-06,\n",
      "         5.3736e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "0.9765\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "# model_path = r'K:\\aio_radar\\lightning_logs\\version_85\\checkpoints\\epoch=74-step=1050.ckpt'\n",
    "# model = RadarGestureNet.load_from_checkpoint(model_path).to(\"cpu\")\n",
    "torch.save(model.state_dict(), 'mti_model.pth')\n",
    "# 加载pth模型\n",
    "model_path = r'model.pth'\n",
    "model = RadarGestureNet(gesture_class=gesture_classes)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# # 预测\n",
    "predictions = model(torch.rand(range_profile.shape[0],30,64),torch.rand(range_profile.shape[0],30,64),torch.rand(range_profile.shape[0],30,64))\n",
    "# \n",
    "# predictions = model(torch.rand(1,30,64),torch.rand(1,30,64),torch.rand(1,30,64))\n",
    "\n",
    "\n",
    "predictions = model(range_profile, speed_profile, angle_profile)\n",
    "print(predictions)\n",
    "\n",
    "# # 准确率\n",
    "print(torch.sum(one_hot_to_label(predictions) == one_hot_to_label(labels)).item() / len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(range_profile.shape)\n",
    "# # 绘制range_profile\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(range_profile[0,3,:])\n",
    "# for i in range(range_profile.shape[0]):\n",
    "#     for j in range(range_profile.shape[1]):\n",
    "#         plt.plot(range_profile[i,j,:])\n",
    "# plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
